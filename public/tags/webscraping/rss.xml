<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
      <title>kyoheiu.gitlab.io - WEBscraping</title>
        <link>https://kyoheiu.gitlab.io</link>
        <description>personal notes</description>
        <generator>Zola</generator>
        <language>ja</language>
        <atom:link href="https://kyoheiu.gitlab.io/tags/webscraping/rss.xml" rel="self" type="application/rss+xml"/>
        <lastBuildDate>Thu, 14 Jan 2021 00:00:00 +0000</lastBuildDate>
        <item>
            <title>JuliaによるWebスクレイピング（簡易版）</title>
            <pubDate>Thu, 14 Jan 2021 00:00:00 +0000</pubDate>
            <link>https://kyoheiu.gitlab.io/post/juliascraping/</link>
            <guid>https://kyoheiu.gitlab.io/post/juliascraping/</guid>
            <description>&lt;p&gt;Juliaで特定のWebページの更新日のみを取得するスクリプト。試し書きに近いのであしからず。&lt;&#x2F;p&gt;
&lt;pre&gt;&lt;code&gt;using HTTP

url = &amp;quot;http:&amp;#x2F;&amp;#x2F;example.com&amp;quot;

function main()
   head = HTTP.head(url)
   lastmod = head.headers[6]
   println(lastmod)
end

@time main()
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;pre&gt;&lt;code&gt;julia&amp;gt; include(&amp;quot;scraping.jl&amp;quot;)
&amp;quot;Last-Modified&amp;quot; =&amp;gt; &amp;quot;Thu, 10 Dec 2020 00:53:40 GMT&amp;quot;
  2.787799 seconds (9.54 M allocations: 479.701 MiB, 6.53% gc time)
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;関数定義部分はもうちょっとチェインっぽい感じでかっこよく書ける気もする。
使用するライブラリはHTTP.jlのみ。head.headersはarrayを返すので、[6]でarray中の要素を指定している（Juliaは1からカウント）。実際、head.headersと番号を指定せずにおくと&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;julia&quot; class=&quot;language-julia &quot;&gt;&lt;code class=&quot;language-julia&quot; data-lang=&quot;julia&quot;&gt;julia&amp;gt; head.headers
8-element Array{Pair{SubString{String},SubString{String}},1}:
           &amp;quot;Date&amp;quot; =&amp;gt; &amp;quot;Thu, 14 Jan 2021 20:35:31 GMT&amp;quot;
   &amp;quot;Content-Type&amp;quot; =&amp;gt; &amp;quot;text&amp;#x2F;html&amp;quot;
 &amp;quot;Content-Length&amp;quot; =&amp;gt; &amp;quot;9480&amp;quot;
     &amp;quot;Connection&amp;quot; =&amp;gt; &amp;quot;keep-alive&amp;quot;
         &amp;quot;Server&amp;quot; =&amp;gt; &amp;quot;Apache&amp;quot;
  &amp;quot;Last-Modified&amp;quot; =&amp;gt; &amp;quot;Thu, 10 Dec 2020 00:53:40 GMT&amp;quot;
  &amp;quot;Accept-Ranges&amp;quot; =&amp;gt; &amp;quot;none&amp;quot;
           &amp;quot;Vary&amp;quot; =&amp;gt; &amp;quot;Range,Accept-Encoding&amp;quot;
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;となる。&lt;&#x2F;p&gt;
&lt;p&gt;取得したHTMLをさらにパースしてbodyやh1など特定のタグの内容を抽出したい場合はGumbo.jlを用いる。&lt;&#x2F;p&gt;
&lt;p&gt;恐ろしく簡潔だが、ちょっと時間がかかりすぎのような気もする。ただ他言語と比較して、ということをやるまでの気力はないので、ここまでにしておきます。&lt;&#x2F;p&gt;
&lt;h4 id=&quot;jlhuairunoshi-xing&quot;&gt;.jlファイルの実行&lt;&#x2F;h4&gt;
&lt;p&gt;ちょっと困ったのが.jlファイルの実行について。REPLモードでincludeして関数を叩く分には何の問題もないのだが、どちらかというとREPLモードに入らず直接ターミナルで実行ファイルを叩く形のほうが好みなので、方法がないか探した。
公式にはターミナルで.jlファイルのあるディレクトリにcdし、julia hoge.jlで実行可能だが、この際、.jlファイルはmoduleとしてではなく、上記のように&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;必要な関数定義&lt;&#x2F;li&gt;
&lt;li&gt;実行したいスクリプト&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;のみを記述する。ちなみにこの場合、REPLモードでも、上記のようにinclude(&amp;quot;hoge.jl&amp;quot;)のみで実行することはできる。
逆にmodule化してしまうと、julia hoge.jlでもinclude(&amp;quot;hoge.jl&amp;quot;)でもスクリプトが実行されることはない（REPLモードの場合は、スクリプト部分がシンタックスエラーと判定されてしまう）。&lt;&#x2F;p&gt;
</description>
        </item>
        <item>
            <title>HaskellによるWebスクレイピング</title>
            <pubDate>Sun, 23 Aug 2020 00:00:00 +0000</pubDate>
            <link>https://kyoheiu.gitlab.io/post/haskellscraping01/</link>
            <guid>https://kyoheiu.gitlab.io/post/haskellscraping01/</guid>
            <description>&lt;p&gt;RSSを吐かず、実際に訪れないと更新されたかどうか確認できないWebサイトの情報を追うために、ささやかだがスクレイピング・プロジェクトを作った。
流れとしてはこういう感じになる。&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Haskell(&lt;code&gt;scalpel&lt;&#x2F;code&gt;)により、更新情報の通知エリアのみを抜き出す&lt;&#x2F;li&gt;
&lt;li&gt;抜き出してきた情報をローカルの&lt;code&gt;update.txt&lt;&#x2F;code&gt;の中身と比較し、同一であれば何もしない、異なっていればそれを&lt;code&gt;update.txt&lt;&#x2F;code&gt;に上書きした上でSlackのチャンネルへ通知（投稿）&lt;&#x2F;li&gt;
&lt;li&gt;このプログラムを、linux起動時にネットワークを確保した上で走るように&lt;code&gt;systemd&lt;&#x2F;code&gt;を使って.serviceを作成&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;この記事では(1)と(2)の&lt;code&gt;update.txt&lt;&#x2F;code&gt;の上書きまでをまとめる。続く部分は以下の記事に。&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;HaskellによるWebスクレイピング（この記事）&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;kyoheiu.gitlab.io&#x2F;post&#x2F;posttoslack&#x2F;&quot;&gt;HaskellでSlackに投稿する&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;kyoheiu.gitlab.io&#x2F;post&#x2F;systemd-service&#x2F;&quot;&gt;systemdを使ってプログラムを定期実行する&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;code&gt;Main.hs&lt;&#x2F;code&gt;は以下の通り。&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;hs&quot; class=&quot;language-hs &quot;&gt;&lt;code class=&quot;language-hs&quot; data-lang=&quot;hs&quot;&gt;{-#LANGUAGE OverloadedStrings#-}

module Main where

import qualified Data.ByteString.Char8 as B
import Text.HTML.Scalpel
import Data.Maybe
import Lib

url :: URL
url = &amp;quot;http:&amp;#x2F;&amp;#x2F;example.com&amp;quot;

filePath :: FilePath
filePath = &amp;quot;update.txt&amp;quot;

data NewsText
    = NewsText { time :: B.ByteString
               , contents :: B.ByteString } deriving (Show,Read,Eq)

main = do
    new &amp;lt;- scrapeURL url information
    old &amp;lt;- B.readFile filePath
    let new2 = B.pack $ show $ fromJust new
    if new2 == old then print &amp;quot;no update.&amp;quot;
                   else do
                       Lib.sendUDMessage
                       B.writeFile filePath new2
    where
            information :: Scraper B.ByteString [NewsText]
            information = chroots (&amp;quot;div&amp;quot; @: [hasClass &amp;quot;information&amp;quot;]) newsTexts

            newsTexts :: Scraper B.ByteString NewsText
            newsTexts = do
                time &amp;lt;- text $ &amp;quot;dt&amp;quot;
                contents &amp;lt;- text $ &amp;quot;dd&amp;quot;
                return $ NewsText time contents
&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;HaskellにおけるWebスクレイピング用のライブラリは他にいくつかあるようだけれど、&lt;code&gt;scalpel&lt;&#x2F;code&gt;は豊富な機能を持ち、たいていのことは可能という印象を受ける。今回の作業においてはオーバーキル気味かもしれないが、一回いじっておくとわりとすんなり他のケースに対しても応用できる、素直なライブラリのように感じた。&lt;br &#x2F;&gt;
&lt;code&gt;scalpel&lt;&#x2F;code&gt;を使う部分は初心者にも大して難しくなかった。&lt;code&gt;div&lt;&#x2F;code&gt;の&lt;code&gt;class&lt;&#x2F;code&gt;でまず絞り、さらにそこからタグで絞って抜き出す、という感じだ。きちんとやるならもっとエラーケースについて考えないといけないはずだが今回はパス。&lt;code&gt;scrapeURL&lt;&#x2F;code&gt;は&lt;code&gt;StringLike str =&amp;gt; URL -&amp;gt; Scraper str a -&amp;gt; IO (Maybe a)&lt;&#x2F;code&gt;となっているので、仮に該当するタグが消失していても&lt;code&gt;Nothing&lt;&#x2F;code&gt;が返ってくる。シンプルだが&lt;code&gt;Maybe&lt;&#x2F;code&gt;の威力を感じる部分。&lt;br &#x2F;&gt;
結局未解決なのは、抜き出してくるテキストに日本語が含まれている場合、&lt;code&gt;Data.Text&lt;&#x2F;code&gt;系を採用しても正確に日本語を拾えない点。これはこちらの文字列の拾い方が悪いのか、対象のWebサイトの仕様なのかよくわからない（要調査）。今回の狙いは更新の有無のみをテキストの「イコールorノットイコール」で判別し、通知する、というものなので、文字化けは許容範囲と判断した。&lt;br &#x2F;&gt;
その上で、&lt;code&gt;Data.Bytestring.Char8&lt;&#x2F;code&gt;を採用しているのは、&lt;code&gt;String&lt;&#x2F;code&gt;はパフォーマンス上一応避けておきたいというのと、過去のテキストと現在のテキストそれぞれを読み込む際のすり合わせのしやすさから。&lt;code&gt;now&lt;&#x2F;code&gt;と&lt;code&gt;old&lt;&#x2F;code&gt;の型が違っていると、内容が同じでも違うものと判断してしまうので、型をちゃんと見なくてはいけない。Visual Studio Codeのエクステンション&lt;code&gt;Haskell&lt;&#x2F;code&gt;に最近&lt;code&gt;haskell-ide-engine&lt;&#x2F;code&gt;が統合されたので、型チェックにはとても有用だった。&lt;&#x2F;p&gt;
</description>
        </item>
    </channel>
</rss>
